跨模态对齐中的文本引导动态的视觉空间精简
---


## **COPA: Efficient Vision-Language Pre-training Through Collaborative Object- and Patch-Text Alignment**  
arXiv:2308.03475 

在 ViT 架构下，用 patch-text alignment 方法学习一个小型文本感知 patch 检测器（3 层小 MLP，用现成的 object/region 数据自动生成 patch 级标签 1=与文本物体重叠，0=无关，再用二分类交叉熵去训练），使模型能挑选出对文本最重要的图像 patch，然后对这类 patch 做视觉-文本对齐。
**视觉和空间层次关系**：通过文本提示自动挑选最有对齐价值的区域，加强局部细节对齐。
可以借鉴这种方式，用 caption 引导筛选 tile，而不是直接统一处理。

---


## **BUS: Efficient and Effective Vision-Language Pre-training with Bottom-Up Patch Summarization**
arXiv:2307.08504
在 ViT backbone 中嵌入两阶段机制：先用一个文本可感知 patch selector（TSPS打分文本 [CLS] 向量 t_cls 与每个 patch 向量 vᵢ 拼起来 → MLP → Sigmoid → 对齐分数 aᵢ 只在transformer某一层这么做）粗略选 token，再通过 Patch Abstraction Decoder（把 ViT 输出的 u 个 patch 再按 ǎᵢ 对齐分数 选 TOP-s，Top-s patch 作为 query，与 ViT 输出的全部 u 个 patch 做 cross-attention。经过两层轻量 Transformer，压缩为s个视觉token。） 把选出的 token 做语义压缩。
**视觉和空间层次关系**：从细粒度 patch 自动聚合为高层 semantic summaries，类似把 tile + 多层特征统一成浓缩视觉表征。
可以借鉴到视觉空间层级融合（tile → summary token）

---

## **MADTP: Multimodal Alignment-Guided Dynamic Token Pruning**
arXiv:2403.02991

在 VLT 模型中动态 prune 不重要 token，并通过 multimodal 对齐信号判断哪些 token 应保留，哪些可以删减。 动态剪枝。
**视觉和空间层次关系**：动态决定哪些 tile/patch 是有意义的跨模态对齐单位，从而减少冗余，保留有效对齐部分。
用 caption 与 tile 之间的对齐分数来动态选择或删减 tile。

---

## **VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment**
arXiv:2210.04135

将图像输出的n个patch token以及文本输出的m个单词token作为集合连边，边权重为cosine相似度。
**视觉 & 空间层次关系**：用 caption 引导 patch 与词 alignment，而不依赖 bounding box 注释。
 caption → tile 对齐——无框监督方法，赋予模型识别哪块 tile 与 caption 匹配。

---


* **文本引导视觉空间选择**（COPA、VoLTA）：用 caption 来关注图像中与文本相关的区域。
* **从多层 patch 融合为 summary token**（BUS）：减少数据，聚集有效信息。
* **动态 token 管理**（如 MADTP）：能够使模型在对齐过程中动态选择和剪枝token。


