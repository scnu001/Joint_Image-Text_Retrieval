# 视觉编码器详解：Tiling + 多尺度融合

## Tiling的目的

* 通过tiling，分割图片的方式，在保留局部细节（表格单元、坐标轴标签、公式标注、微小图形结构）的同时，不丢失全局布局语义（图是什么类型、整体趋势）。
* 支持将视觉信息与 caption / 文本块在多粒度层次上对齐（浅层强调纹理/结构，深层强调语义）。

---

## tiling的策略选择

### 固定网格（Grid）
eg：DeepSeek-VL2，LLaVA-Mini，LLaVA-NeXT，UReader
* 将原始图像按行列均匀划分为 $M \times N$ 个 tile。
* 常用参数：tile 长边尺寸 S（ 384 或 448），若原图任意长宽则对短边按 S 缩放并在长边做滑窗/裁剪。
* 实现最简单、易并行；但是可能把关键小物体切在边界上。

### 重叠滑窗（Sliding windows）
eg：基本没有模型采取切图重复
* 使用步长 stride = (1 - ρ) \* S，ρ 为重叠比例（常设 ρ∈\[0,0.25]）。
* 适合需要更高召回的小目标，但会产生更多 tile。


### 内容感知/候选区域（Adaptive / ROI）

* 先用轻量检测模型 / caption 引导的区域提取（例如用 caption embedding 与下采样特征做快速点乘得分），选择 top-K 高分区域作为 tiles。以patch-level与caption判断相关性，在通过合并patch为tile。
* 计算集中在重要区域；但是可能需要训练额外打分器。

### 想法

* 结合：一张全局缩略图 + 固定少量网格 tile + caption-guided top-K作为补充。
* 处理不同图类型（表格 vs 流程图 vs 实验图）时，可动态切换 tile 数或 overlap。

---

## Tile 编码

### 预处理

* 每个 tile 缩放到提取特征的网络要求的输入尺寸，常见为 224/384/448。
* 对全局图像也做统一缩放到某个固定分辨率（例如 1024 的长边）。

### 特征提取网络与层次输出

* 使用视觉 encoder（eg ViT / ResNet / Swin）抽取多层特征：选择浅层（边缘/纹理）、中层（局部模式）、深层（语义）作为多尺度表示。
* 记第 \( l \) 层输出为 \( v_i^{(l)} \in \mathbb{R}^D \)（tile \( i \) 的第 \( l \) 层表示），全局图为 \( v_g^{(l)} \)。

### 投影与归一化

* 每个 $v$ 经过线性投影到公共向量维度 $d$（例如 $d=768$）：
  $\tilde{v} = \mathrm{Proj}(v) \in \mathbb{R}^d$
* 在做余弦相似度前进行 L2 归一化：$\hat{v} = \tilde{v} / \|\tilde{v}\|_2$。


---

## Tile位置信息

### 绝对/相对坐标编码

对每个 tile i，构造位置信息向量：

* 归一化中心坐标 \( x_i, y_i \in [0,1] \)（相对于图像宽高）；
* 归一化宽高 \( w_i, h_i \in [0,1] \)；
* 可选：tile 在文档中对应页码或像素缩放因子。

把位置向量 \( p_i = [x_i, y_i, w_i, h_i] \) 经过 MLP 或可学习位置嵌入投影并与 tile 特征拼接或相加：
\[
v_i'^{(l)} = \mathrm{LayerNorm}(\tilde{v}_i^{(l)} + \mathrm{PE}(p_i))
\]
其中 PE 可以是可学习向量或 sinusoidal 编码。



---
## OCR 与视觉-文本融合
### OCR 流程
* 对整张图或每个 tile 做 OCR（分 tile 做，这样更容易定位词的 tile）。得到 tokens \( w_j \) 及其 bounding boxes。
* 将 OCR token 用文本 encoder（或小型 embedding 表）映射到同维度 \( d \)：\( o_j \in \mathbb{R}^d \)。
### 整合方式
* **方式 A：直接合并到 tile 表征**  
  对某 tile \( i \)，取其含有的 OCR token 的平均或 attention 聚合为 \( o_i \)，并拼接/加权和到 \( v_i' \)：
  \[
  v_i'' = \mathrm{MLP}([v_i',\,o_i])
  \]
* **方式 B：作为键值的 cross-attention**  
  在后续的融合 Transformer 中把 OCR token 作为附加的 value/key，与 caption token 一起做 cross-attention，使 OCR 信息在对齐时被显式利用。

---

## Tile 选择与压缩
### Top-K 选择（基于 caption 相关性）
* 计算 caption embedding $C$（文本 encoder 输出），对每个 tile 计算相关性 $\rho_i=\mathrm{sim}(C,v_i')$，取 Top-K tiles 进入精排（K 例如 4–8）。


###  压缩 / 量化

* 离线对 tile 向量做 PCA / product quantization，压缩到较低维或使用向量索引库（FAISS）加速 ANN 检索。
* 在精排阶段恢复原始高维向量或使用低维近似完成快速评分。


---

## 多尺度融合（把多层 tile + 全局融合成一个可对齐的视觉表征）

我把常用方案分成 4 类 —— 都是可实现的，你可以根据目标和资源任选或比较。

### 方案 1：拼接后线性投影（简单）

* 对每个 tile 抽取 L 层（浅/中/深）的投影 $\hat v_i^{(1)},\dots,\hat v_i^{(L)}$，并与全局 $\hat v_g^{(l)}$ 拼接：
  $u_i = \mathrm{Concat}(\hat v_i^{(1)},\dots,\hat v_i^{(L)},\,\hat v_g^{(1)},\dots,\hat v_g^{(L)})$
  然后通过 MLP 或线性层降维到 d：
  $v_i^{\text{fused}} = \mathrm{Proj}(u_i)$
* 实现简单；但是参数多、可能冗余。

### 方案 2：层加权求和（轻量）

* 给每层一个可学习权重 $\alpha_l$（或条件化的 gate，见 7.4），做加权和：

  $v_i^{\text{fused}}=\sum_{l=1}^L \alpha_l \hat v_i^{(l)} + \sum_{l=1}^L \alpha_l^g \hat v_g^{(l)}$
* 再做 layer-norm + 投影。
* 参数少，运行速度快。

### 方案 3：Attention-based 融合（表达力强）

* 把不同层的表示视为一组 tokens，使用一个小 Transformer（或 SetTransformer）在 layers×tiles 的维度上做自注意力聚合，允许模型学习跨层、跨 tile 的交互：

  * Input tokens: ${\hat v_i^{(l)}}_{i,l}$ + $\hat v_g^{(l)}$
  * 输出为若干聚合向量，最后做 pooling（[CLS]或 mean）得到 $V^{\text{agg}}$。
* 能够捕捉复杂跨层交互；计算量大。


### 深度/条件门控（Caption-aware gating）

让层权重 $\alpha_l$、tile 权重 $\beta_i$ 条件化于 caption embedding $C$（或二者共同），即：
  $\alpha = \mathrm{softmax}(\mathrm{MLP}([\hat v_g^{(1)},\dots,\hat v_g^{(L)}, C]))$
  $\beta = \mathrm{softmax}(\{\mathrm{sim}(C,v_i')\}_{i=1}^N)$
* 最终融合：
  $V^\text{fused} = \sum_{i=1}^N \beta_i \Big(\sum_{l=1}^L \alpha_l \hat v_i^{(l)}\Big)$
* 这样模型可以为不同的查询（caption）自动选择最有用的视觉层与 tile（比如对表格更多依赖浅层/纹理，对示意图更多依赖深层语义）。


---

## 与文本对齐的接口（如何让视觉向量可直接与文本块相似度计算）

* 将 $V^\text{fused}$ 经过线性头映射到文本公共空间（共享维度 d），再 L2 归一化；文本块 $B$ 也归一化。
* 相似度用余弦： $\mathrm{sim}(V,B) = V^\top B$。
* 如果采用 caption-guided精排，可在计算 $\beta_i$ 时复用 $\mathrm{sim}(C, v_i)$ 做初步筛选。

---

##  训练的监督策略

### 对比学习（Image–Block）

* InfoNCE 风格：对图像（或图像+caption）与正样本块做对比，负样本可来自同批其他块或硬负挖掘：
  $\mathcal{L}_{v} = -\log\frac{\exp(\mathrm{sim}(V^\text{fused},B^+)/\tau)}{\sum_{b\in\mathcal{N}}\exp(\mathrm{sim}(V^\text{fused},B)/\tau)}$

### Tile-Caption 对齐（多实例损失）

* 将tile对齐到caption的某些词/短语，可把tile视为实例，用MIL（multi-instance learning）损失促使至少一个tile与caption高相似度：
  $\mathcal{L}_{\text{MIL}} = -\log\frac{\max_{i}\exp(\mathrm{sim}(v_i^{\text{fused}}, C)/\tau)}{\sum_{b\in\mathcal{N}}\max_{i}\exp(\mathrm{sim}(v_i^{\text{fused}}, C)/\tau)}$

### OCR 词级对齐

* 强制 OCR token 与文本块关键词相似，或者把 OCR token 作为额外正样本加到对比池里，提升术语匹配（尤其是表格/标签丰富场景）。

---

## 推理流程

1. 对整库文档离线计算并存储文本块向量 $B_m$（量化/索引）。
2. 在线：给定 $(I,c)$：

   * 计算 caption embedding $C$ → 粗召回候选块（ANN）
   * 切分 tile（或加载 precomputed tile crops） → 编码并生成 $V^\text{fused}$（可先只对 top-K tiles 编码）
   * 在候选集合上计算相似度并重排序。
3. 缓存策略：常见配图可缓存 tile embeddings 与 fusion 中间结果，减少重复计算。

---

## 推荐超参数

* 全局图长边：1024；tile 尺寸 S：384 或 448；重叠 ρ：0–0.25；N（初始）≈6–12；Top-K（caption-guided）≈4–8。
* 抽取层数 L：3（浅/中/深）；向量维度 d：768（可选 512）。
* 对比损失温度 τ：0.05–0.1；学习率：视觉头（projection）1e-4–1e-5；若微调 backbone，使用更小 lr。
* 候选规模 K（粗召回）：200；精排 Top-k 输出：10–50。

---

## 消融实验

* 有/无 OCR 的效果差异；OCR 单独作为检索信号的提升量。
* 固定网格 vs 重叠滑窗（感觉性能差异不大，效率太低） vs caption-guided top-K（召回/精排对比）。
* 融合方式对比：拼接投影 / 层加权 / attention-based（评估 mAP / Recall\@k）。
* 是否使用深度条件化 gate（α、β）：验证是否对不同图类型自适应。
* MIL tile-level 损失是否提升小目标对齐（表格/图注细粒度）。

---

