1. **视觉层次融合（$V_{fused}$）** 通过融合 Vision Transformer（ViT）的浅层、中层、倒数第二层和最终层输出，构建包含丰富语义与结构信息的多粒度视觉表征。特别强调中层语义特征与倒二层的空间结构信息，以增强细节表达与高层理解的结合。 
2. **空间层次融合（$S_{fused}$）** 针对高分辨率图像采用分块（tiles）与缩略图（thumbnail）相结合的策略。各 tile 共享 ViT 编码器，提取其中层与倒二层 token 并投影得到局部高分辨率表征 ${t_i}$；同时利用缩略图保持全局语义一致性，最终融合局部细节与全局结构，形成统一的空间增强表征。 
3. **视觉+空间统一融合（$I_{fused}$）** 引入文本 caption 作为引导信号(在空间层次和数据层次自己融合的时候加，还是在融合之后加？)，将 $V_{fused}$ 与 $S_{fused}$ 进行条件融合，生成受文本语义调制的统一图像 embedding，服务于后续跨模态对齐任务。 
4. **文本层次建模** 构建多粒度文本表征，包括词级（$T_{word}$）、句子级（$T_{sent}$）和文本块级（$T_{block}$），以支持不同粒度的语义理解与匹配需求，尤其适用于长文本与复杂图像内容的细粒度关联。 
5. **跨模态对齐** 在最终阶段实现双向跨模态对齐：全局层面完成图像与文本块的精准检索，局部层面支持句子与图像 tile 的细粒度对应，从而实现从整体到局部的全面语义匹配。
---
## A. 视觉层次融合（ViT 不同层 → $V_{fused}$）
1. **哪些 ViT 层要选？（具体层号/层区间）**
   *原因：* 不同层的语义差异大，层的选取直接影响细粒度和全局语义的平衡。
2. **如何对不同层输出做归一化/标准化（LayerNorm、BN、投影等）？**
   *原因：* 层间分布不一致会使融合权重学习不稳定性。
3. **融合前是否需要对齐投影头（公用/分层）？**
   *原因：* 不同层维度与语义尺度差异可能需要统一表示空间，否则相似度判别难以一致。
4. **门控/权重机制(不同层加权融合时)应是静态参数还是条件化（例如基于 caption/全局统计）？**
   *原因：* 静态与动态权重在适应不同输入场景（比如说表格/示意图/曲线）时表现不同。
5. **是否需要对中层/倒二层单独设计中间监督（层级 loss）？**
   *原因：* 没有中间监督可能导致层被“平均化”而丢细节，监督方案会影响层的职责划分。(类似浅层 ←→ token/像素级，中层 ←→ 句级，倒二层 ←→ 段落/区域级)
好的，这是将您提供的所有问题中的“为什么问”部分替换为“原因”后的版本，格式和内容保持一致。
6. **层间交互要用简单加权、跨层注意力还是其它聚合方式？**
   *原因：* 不同聚合方式对长序列复杂度、信息保留和可解释性的影响不同。
7. **融合后有没有必要保留层识别信息（layer token / positional tag 类似位置编码或者标识符代表来自哪个层）？**
   *原因：* 如果丢失来源信息，后续模块可能无法区分来自哪一层的信号，影响故障分析。
8. **如何衡量融合后是否保留了“细节证据”（可用于定位/解释）？**
   *原因：* 检索任务需要不仅相似度高，还要可追溯到具体 tile/句子(文本块对应图片的哪一个tile 个人感觉如果只对应一部分 也应该算匹配)，指标需定义。
9. **在训练中如何避免某层主导（collapse 到单一层）？**
   *原因：* 单一层主导可能会让其他层特征融合中消失。
10. **是否需要对不同层采用不同的学习率/正则化策略？**
    *原因：* 各层表征，如浅层梯度较小，会被降低权重最小化损失。
## B. 空间层次融合（Tiles + Thumbnail → $S_{fused}$）
11. **tile 的尺寸、重叠比例和滑窗策略如何确定？**
    *原因：* 尺寸/重叠直接影响细节能否被捕获与计算量(目前主流的办法是不重叠，并且尺寸为vit的384)。
12. **tile 的选择策略是均匀网格还是只关注和caption相关的？具体如何评估选择质量？**
    *原因：* 选择策略决定召回/效率权衡(internVL采用的是均匀网格)。
13. **tiles 在编码器中是否共享权重（同一 ViT）或使用独立分支？**
    *原因：* 共享会节省参数但可能欠拟合某些tile分布；独立分支增加成本且易过拟合(主流办法是共享权重)。
14. **tile 的位置/几何信息如何编码并在融合中使用？（绝对/相对坐标）**
    *原因：* 位置编码影响局部-全局语义的组合与定位能力。
15. **tile 聚合成局部 summary 的方法（pooling/attention/transformer）应如何选？**
    *原因：* 不同聚合影响局部语义保留和下游成本(主流办法是mean pooling但是是在大规模图文对中训练的)。
16. **缩略图和 tiles 的融合顺序和融合策略是否固定或可变？**
    *原因：* 先 global→local 或先 local→global 会改变信息流与梯度传播(internvl——并行编码，平均池化)。
17. **如何保证缩略图保持的全局语义不会在与 tile 融合时被压制？**
    *原因：* 融合不当会丢全局信息，影响块级匹配(用缩略图作为query，tile作为key和value？)。
18. **tile 聚合中如何处理 OCR 文本（token-level vs aggregate）及其对齐方式？**
    *原因：* OCR 信息是重要细粒度信号(OCR如果返回错误的结果怎么处理，是要作单独的token列拼接视觉token还是整体作为向量和视觉向量融合)。
19. **tile 的负样本与正样本定义（训练时）如何构建？**
    *原因：* tile 的 supervision 采样策略会影响 MIL 类损失的收敛与稳定性(只要匹配上，该图片的所有tile，所有判定为正例，同batch其余图片的tile作为负例)。
20. **当 tile 数量很多时，如何在不丢信息的情况下压缩或筛选？**
    *原因：* 计算/存储限制促使压缩策略(tile分辨率是384*384，tile数量应该不会太多)。
## C. 视觉 + 空间统一融合（条件于 caption → $I_{fused}$）
21. **caption 引导是在融合前注入还是融合后注入（early vs late conditioning）？**
    *原因：* 注入时机决定 caption 对 tile/层的影响范围。
22. **caption 用作query（cross-attention）还是作为 soft conditioning（gate/FiLM 生成缩放和偏移参数类似一个权重）？**
    *原因：* 不同条件化机制对训练稳定性、泛化性和解释性有不同影响。
23. **如何防止 caption 驱动导致模型只根据 caption 找块？**
    *原因：* 若依赖 caption 过强，检索可能失去对图像本身的判断能力，容易作弊(mask caption随机drop out )。
24. **条件化的数值稳定性与梯度流如何保证（caption与视觉梯度冲突）？**
    *原因：* 跨模态条件可能引入梯度方向冲突，影响训练收敛。
25. **是否需要把 caption 信息分层（词/句）注入到不同的视觉层次？**
    *原因：* 不同粒度的语言信息可能最适合不同视觉层次的引导(统一注入可能比较可控)。
26. **融合后 $I_{fused}$ 是否保留可映射到具体 tile/layer 的可解释性？**
    *原因：* 检索应用常需定位句子对应区域，单一向量的话很难保证这种显式的对齐(需不需要这种具体到哪一部分的UI器)。
27. **将caption作为引导信号时如何处理 caption 中噪声**
    *原因：* 过短或含糊的captio容易引导错误的视觉关注。
## D. 文本层次建模（Hi-Transformer）
28. **文本块划分策略（段落/小节/滑窗）如何设定以及对齐粒度如何选择？**
    *原因：* 块的定义直接影响 recall/precision 的平衡；过粗或过细都会影响检索效果(用LLM 设置prompt进行切分)。
29. **句级聚合用何种 pooling（mean/attention/\[CLS]）？对对齐效果的影响是什么？**
    *原因：* 不同 pooling 影响句向量的区分度与可对齐细粒度能力(倾向于采用[CLS]token)。
30. **块内 Transformer 的层数与可学习参数量应如何选？是否需要位置/角色 embedding？**
    *原因：* 模型容量与文本结构化信息对齐表现密切相关(四层，也许可以增加一个主题词)。
31. **文本自监督（Masked language Modeling/句子顺序预测）是否在对齐前必须完成？**
    *原因：* 文本表示的稳定性直接影响跨模态对齐的上限。
32. **如何处理文本中的指代/图表引用（“如图3所示”）对块定位的影响？**
    *原因：* （此处原文未提供原因，已为您补充）指代和图表引用是跨文本块的隐式关联，若处理不当会破坏文本的语义连贯性，导致模型无法正确理解文本与图像的对应关系，从而影响定位的准确性。
33. **是否需要对句/块设置显式重要性先验（比如直接提到图3所示）？**
    *原因：* 若模型无法自己发现显式的匹配词，显式先验会影响召回质量（手动调整权重过大）。
---
## E. 跨模态对齐与损失设计
34. **主对齐损失（InfoNCE）的方向性与采样策略如何定（双向还是单向）？**
    *原因：* 损失形式与负样本采样直接决定检索指标（R\@K、nDCG）的收敛方向(用图像去靠近文本，拿文本作为正负例子)。
35. **局部对齐（句↔tile）应采用 MIL(平均or最大)、Optimal Transport(求两个分布的最优匹配矩阵)还是逐 token 对比？**
    *原因：* 局部对齐损失在计算复杂度与召回精度间权衡不同(如果最终图片模态先融合在对齐文本模态，可能并不需要局部对齐，需要局部对齐的化，MIL)。
36. **是否需要额外的二分类image-text matching损失以辅助 hard-negative(与真例相似的负例)学习？**
    *原因：* ITM(0,1概率)可补充对比学习不容易判断的负例，ITM常用交叉熵，梯度尺度和对比损失不同，会增加训练目标冲突。
37. **对多正样本（一图对应多块/多句）如何在损失中正确处理（log-sum-exp or max/mean pooling ）？**
    *原因：* 错误聚合多正例会导致梯度方向模糊或样本权重偏差(log-sum-exp 温度高趋向于mean，低趋向于max)。
38. **温度参数 τ 是否可学习，负样本温度应否层级化（全局 和局部）？**
    *原因：* 温度影响对比学习，温度低只注重最像正例的负样本，温度高平均考虑所有负样本；在不同层级考虑不同的温度。
39. **是否需要对不同粒度的损失采用动态权重调度（课程式训练）？**
    *原因：* 细粒度与粗粒度对齐难度和收敛速度不同(先放大全局对齐损失，再放大局部对齐损失)。
40. **负样本构建的域（同论文和跨论文）如何平衡？**
    *原因：* 同论文负样本通常比较难，但是任务就是在同论文中检索，所以大部分负样本都要考虑用同论文的。
---
## F. 训练策略与稳定性
41. **是否需要阶段化训练（单模态预训练 → 融合训练 → 跨模态微调）？**
    *原因：* 多任务损失下直接端到端训练可能导致不稳定或梯度冲突。
42. **如何检测并应对不同损失间的梯度冲突（例如用 GradNorm/uncertainty weighting）？**
    *原因：* 多损失共存下，某些目标可能被其他目标主导
    (Uncertainty weighting \[
\mathcal{L} = \sum_i \frac{1}{2\sigma_i^2} \mathcal{L}_i + \log \sigma_i
\]


    GradNorm
    \[ L_{\text{GradNorm}} = \sum_i \left| w_i \cdot \| \mathbf{G}_i \| - \bar{G} \right| \])。
43. **batch size、负样本池（memory bank 将之前batch计算的旧特征储存作为额外负样本使用）与大 batch 对 contrastive 学习的重要性如何权衡？**
    *原因：* 资源受限情况下需要策略保证(可能不需要担心batch过大，因为主要选择的负样本是同一篇论文的，所以负样本数量有限)。
44. **hard negative挖掘的频率/策略要如何设计？**
    *原因：* 频繁训练hard negative可提高精度但也易过拟合或不稳定(与正例数保持一定比例，如1:1；计算hard negative和正样本的相似度，过大的话停止用于训练)。
45. **训练数据标注偏差（论文内结构差异、caption差异）如何会影响模型泛化？**
    *原因：* 数据偏差会导致模型学到不具通用性的对齐规则(位置shuffle,caption同义改写，跨学科论文微调)。
---
## G. 推理、效率与系统工程
46. **离线索引应保存哪些 embedding（块级、句级、tile summary）？**
    *原因：* 存储/检索效率与召回质量(粗排保存图片模态和文本模态向量，精排保存tile和句级向量)。
47. **在线粗排和精排的分工点如何界定（何时计算 tile 级 ）？**
    *原因：* 决定了最终系统的延迟与吞吐能力。
48. **如何保证推理时的浮点/量化(向量计算小数点精度)？**
    *原因：* 量化/加速会改变向量角度分布，影响检索排名。
---
## H. 评估
49. **检索评估指标要包含哪些（R\@K(前k个有无正确答案), nDCG\@K(排序位置加权), MRR(第一个正确结果的平均倒数排名 比如第一个正确结果的排名为2，那么MRR为1/2）？**
    *原因：* 单一指标可能掩盖局部对齐质量或定位能力下降。
50. **如何评估句↔tile 对齐的定位准确度（感觉可能难度较大）？**
    *原因：* 检索任务往往需要定位证据，需设计定位/可解释性指标。
51. **在何种语料分布下做泛化测试（不同领域/不同期刊会议/不同图表类型）？**
    *原因：* 论文图像类型多样，可能需要不同领域的论文进行泛化测试。
---
## I. caption和OCR噪声
52. **当 caption 与图像描述冲突或 caption 非常笼统时，系统如何表现？**
    *原因：* 现实数据中存在噪声 caption，影响召回可靠性。
53. **当 OCR 错误频发（表格/公式场景）时，模型对齐性能降低？**
    *原因：* OCR精度很重要(精度不高是否放弃将OCR结果作为输入)。
54. **会不会出现语义级对齐成功但局部匹配错误(这应该是指的是tile和句级的匹配错误，如果先融合在对齐的话，主要从全局语义出发)模态错配？**
    *原因：* 需要可解释性度量而非仅全局相似度。


---

## 消融实验可能设计的范围

### 1. **层选择（视觉层次融合）**

* **浅层 + 中层 + 倒二层 + 最终层**
  ↔ 分别去掉某一层，看哪个层贡献最大。
* **仅最终层 vs 多层融合**
  ↔ 验证多粒度特征是否真的优于单层高语义特征。
* **中层替换实验**（如第 6 层 vs 第 8 层）
  ↔ 测试中层特征位置的鲁棒性。

---

### 2. **Tile 数量 K（空间层次融合）**

* **K = 1（仅缩略图）**
* **小 K（例如 4 tiles）**
* **大 K（例如 16 / 36 tiles）**
* **自适应 K（根据图像尺寸动态划分）**

---

### 3. **缩略图与 tile 的融合方式**

* **直接拼接 + Transformer**
* **加权平均（静态权重）**
* **门控机制（动态权重分配）**
* **无缩略图（仅 tile）**

---

### 4. **门控机制设计**

* **无门控（简单 concat / mean pooling）**
* **静态门控（可学习参数）**
* **动态门控（依赖输入生成权重，例如 attention-based gating）**
* **层间共享门控 vs 独立门控**

---

### 5. **损失函数组合**

* **仅全局对齐（图 ↔ 文块，InfoNCE）**
* **仅局部对齐（tile ↔ 句子，MIL/OT/Token 对比）**
* **全局 + 局部（固定权重）**
* **全局 + 局部（动态权重，如 uncertainty weighting）**
* **多层监督 vs 单层监督**

---

### 6. **Caption 引导位置**

* **仅在视觉层次融合时引入**
* **仅在空间层次融合时引入**
* **统一视觉+空间后引入**
* **多阶段引入（逐步调制）**
---

### 7. **局部对齐策略**

* **MIL（平均池化 vs 最大池化）**
* **Optimal Transport（分布级对齐）**
* **逐 token 对比**
* **无局部对齐**


---

### 8. **损失权重调度方式**

* **固定权重**
* **手工调度（课程式训练：先全局，后细粒度）**
* **动态调度（GradNorm / Uncertainty weighting / PCGrad）**

---

### 9. **文本侧多粒度建模**

* **仅块级 embedding**
* **块级 + 句级**
* **块级 + 句级 + 词级**
* **自适应选择粒度（动态 gating）**

---

### 10. **正负样本构建策略（尤其 tile ↔ 文本）**

* **所有 tile 均视为正例（只要图像匹配）**
* **仅与 caption 内容相关的 tile 判为正例（mask/attention 指导）**
* **其余 tile 作为弱负例**

---

## 可能的消融实验结构

| 消融维度         | 对比设置              | 目的               |
| ------------ | ----------------- | ---------------- |
| 层选择          | 最终层 / 多层 / 去掉中层   | 验证层次融合贡献         |
| Tile 数量 K    | 1 / 4 / 16 / 动态   | 评估局部细节影响         |
| 缩略图作用        | 无 / 简单拼接 / 门控     | 全局+局部互补性         |
| 门控机制         | 无 / 静态 / 动态       | 动态融合是否必要         |
| 损失函数         | 全局 / 局部 / 全局+局部   | 粒度监督效果           |
| Caption 引导位置 | 融合前 / 融合后         | 文本调制时机效果         |
| 对齐方式         | MIL / OT / token  | 局部对齐方法比较         |
| 损失调度         | 固定 / 课程 / 动态      | 多任务优化稳定性         |
| 文本粒度         | 块 / 块+句 / 全粒度     | 文本侧贡献分析          |
| 正负样本策略       | 全部正例 / 部分正例 / 弱负例 | tile-level 监督稳定性 |


