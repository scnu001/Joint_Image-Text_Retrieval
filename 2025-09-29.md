# 多智能体与文档理解相关研究整理

---

## 1. DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding

### 文本支路
- 保留 LLM 原词表；
- OCR 结果直接喂入子词 token，与坐标无关。

### 视觉支路
- 使用轻量 CNN（Swin-Tiny）抽取 patch 特征；
- 每个 patch 仅 256 维。

### Layout Fusion Module
- 将文本 bbox 中心 (x, y) 量化到 64×64 网格；
- 同一网格内的文本 token 与视觉 patch 在局部 3×3 窗口内进行 cross-attention；
- 网格空位用可学习的「空白 token」补齐，保持 2D 结构。

---

## 2. Exploring Collaboration Mechanisms for LLM Agents: A Study of Linked Large Language Models

- 以辩论为主的协作策略（如“辩论-辩论-反思”）；
- 随和型智能体更容易达成共识（对比：随和 vs. 自信）；
- **3 个智能体协作 3 轮** 是性价比最高的配置。

---

## 3. Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?

- **单智能体 + 强提示（含示例） ≈ 多智能体讨论效果**（多智能体可视为替代示例）；
- 在常识推理、数学应用、逻辑推理任务中验证；
- 多种协作机制对比：
  - **Debate（对称式辩论）**：多个 LLM 回答同一问题；
  - **MAD（Multi-Agent Debate with Roles）**：角色分工（辩论 + 裁判）——表现最差；
  - **ReConcile（加权投票机制）**：
    - 多个 LLM 智能体轮流发言，每轮可修正答案；
    - 最终答案为加权共识，非简单投票；
  - **CMD（Conquer-and-Merge Discussion）**：
    - 分组 → 组内讨论达成共识 → 组间合并共识并跨组讨论。

- **强 LLM 可逐步提升弱 LLM 表现**（通过讨论）。

---

## 4. Evaluating the Collaboration and Competition of LLM Agents

- **能力差异过大的模型协作**：需设立奖励或权重平衡；
- 网络拓扑结构对比：
  - 全连接
  - 星形（有一个整合信息的中心节点）
  - 线性
- **星型结构 + 3 轮交互**：性价比最高；
  - 整合节点设为最强 LLM；
- **能力相同时**：轮流整合性能略降（因 perceived fairness 需求）。

---

## 5. Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions

### 核心挑战（C1–C4）：

| 编号 | 挑战 | 描述 | 开放问题 |
|------|------|------|--------|
| **C1** | 构造的模糊性（Ambiguity in Constructs） | 社会概念（如“关心”、“紧张”）主观且模糊，难以统一建模或标注 | 如何建模标签不确定性？是否可用自然语言描述代替离散标签？ |
| **C2** | 细腻信号（Nuanced Signals） | 社交中语言、语调、表情、姿态等信号极其微妙 | 如何用语言引导视觉/听觉感知？如何构建跨模态社会信号理解模型？ |
| **C3** | 多重视角（Multiple Perspectives） | 不同参与者有不同背景、目标与解读 | 如何建模“视角间依赖”？是否需引入博弈论或共识机制？ |
| **C4** | 代理性与适应性（Agency & Adaptation） | AI 需具备目标导向、主动学习与社交适应能力 | 如何定义“社交目标”？如何平衡任务完成与社交适应性？ |

---

## 6. Teaching Language Model Agents How to Self-Improve

- **Self-Critique Prompt**：让模型对自己输出写“差评”；
- **Refine Prompt**：将“差评”反馈回模型生成改进结果；
- 循环 T 次，直到 critique 不再挑出问题；
- 使用最后一次最佳样本作为正样本；
- 引入 **verbalized confidence**：模型在 critique 阶段输出 0–100 分（记为 \( r_\theta \)）。

- 自我改进 **5 轮后增益饱和** → 建议 **early-stop@3** + 熵正则；
- 开放任务中模型易自评过高，提前停止改进；
- **模型越大，绝对收益越大**：
  - 3B 模型：自评分 <62 无法有效自训练，需 ≥75；
  - 30B 模型：81% 样本有效 vs. 3B 的 38%。

---

## 7. Reflective Multi-Agent Collaboration based on Large Language Models

### 三阶段流程：

1. **Collaborative Step**  
   - 各 agent 按角色 prompt 独立生成答案 → 投票得临时决议。

2. **Reflective Step创新点**  
   - 所有 agent 查看真值或环境反馈后，开始反思：
     - 我上一轮错在哪？
     - 其他 agent 谁对？为什么？（关键 消融性能差异最大）
     - 我下次要改什么？
   - 反思内容 append 到各自系统 prompt，形成「长期记忆」。

3. **Corrective Step**  
   - 带更新后的 prompt 重跑同一任务（或下一帧），群体答案自动修正。


- 先用真实数据集跑两轮；
- 后续切换为轻量打分器或 GPT-4 打分；
- **稀疏奖励场景**（仅最终 0/1）：
  - 采用“事后重定目标 + 再算 advantage”策略，
  - 将稀疏奖励转化为密集奖励，输入反思模块。

---

## 8. AgentSquare: Automatic LLM Agent Search in Modular Design Space

将“搭建 LLM Agent”转化为 **神经网络架构搜索（NAS）** 问题，搜索空间分为四模块：

| 模块 | 内容 | 示例 | 贡献度 |
|------|------|------|--------|
| **1. Role Block** | “谁” | Planner, Coder, Critic, Reviewer, Executor | **62%** |
| **2. Prompt Style Block** | “怎么说” | CoT, ReAct, Reflexion, Role-Play, Mini-Style | **21%** |
| **3. Tool Block** | “用啥” | Python, Search, DB, Browser, Calculator | **12%** |
| **4. Edge Block** | “跟谁说话” | 单向、双向、Group-Net、Star-Net、No-Link | — |


- **Role + Edge 拓扑结构** 贡献最大（62%）；
- 搜索策略建议：
  1. 先搜 **通信拓扑（谁跟谁说话）**；
  2. 再搜 **Prompt 风格**；
  3. 最后微调温度等连续超参。
- **约 150 次推理** 即可确定最优参数配置。