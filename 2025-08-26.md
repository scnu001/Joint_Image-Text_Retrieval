LLaVA-NeXT (LLaVA-UHD)	任意长宽比 + 任意分辨率	图像被动态切成 ≤9 个 448×448 tile；原图 336×336 缩略图一起输入

DeepSeek-VL2	2×2、3×3 等规则网格切图 + 全局缩略图	固定 384×384 tile；支持 1×1 到 3×3 网格

NaViT (Google)	原生可变分辨率 ViT，无需 resize	把图像拆成任意大小的 patch 后直接 pack 成一个长序列	

LLaVA-Mini	只用一个 vision token 的极端压缩	高分辨率图切成 4 个子图，各子图编码后再融合	

UReader	面向文档 OCR 的动态分辨率	将图像切成 224/336/448 三档 tile，tile 间无参数交互这里的把一张高分辨率图像切成若干 patch／tile／crop，每个子图单独用 ViT 编码，再把得到的 token 序列与全局信息（例如原图低分辨率版本）一起送入 LLM。

--------------------------  

###  1Tiling  

**Tiling是解决Any Resolution问题的一种经典方案**，代表性工作包括UReader、LLaVA-Next、InternVL 1.5、BLIP-3等。其核心思想是通过**预设固定尺寸的子图（tile）与分辨率模板**，将任意分辨率图像转化为可并行处理的标准化输入，具体实现如下：  

---

#### **核心流程**  
1. **预设参数定义**  
   - **Tile尺寸**：固定子图分辨率（如 `384×384`）  
   - **尺度模板**：预设4种宽高比组合（`1:1`, `1:2`, `2:1`, `2:2`），对应不同分辨率：  
     | 尺度模板 | 预设分辨率 |  
     |----------|------------|  
     | 1:1      | 384×384    |  
     | 1:2      | 384×768    |  
     | 2:1      | 768×384    |  
     | 2:2      | 768×768    |  

2. **图像适配与切分**  
   - **Step 1: 分辨率映射**  
     将输入图像匹配到**最接近的预设分辨率**（如输入 `500×800` → 映射至 `384×768`）。  
     → *图示：原始图像按宽高比缩放至目标分辨率（如1:2模板）*  
   - **Step 2: Tile切分**  
     将映射后的图像切分为多个 `384×384` 子图（如 `384×768` → 切为2个子图）。  
     → *图示：图像被网格化切分为固定尺寸子图（含边界填充处理）*  
   - **Step 3: 增补全局信息**  
     将原图**整体缩放至 `384×384`**，作为"全局摘要图"与子图拼接。  

3. **并行化输入**  
   - 将所有子图 + 全局摘要图在 **Batch维度拼接**，一次性输入ViT：  
     $$ \text{Input} = [\text{Tile}_1, \text{Tile}_2, ..., \text{Tile}_n, \text{Global}_{384×384}] $$  
     → *图示：子图与全局图并行送入ViT，输出特征向量拼接*  

---

#### **本质与特性**  
- **本质**：将任意分辨率图像**强制映射到预设分辨率网格**，通过切分实现"伪任意分辨率"处理，**仍属于固定形状的Tokenization机制**（依赖预设tile尺寸）。  
- **核心目标**：突破ViT对固定输入分辨率的限制，**实现多分辨率图像的并行推理**。  

---

#### **局限**  

在捕获全局信息上，仅仅是用低分辨率图（如 `384×384`）作为全局信息，**无法有效保留原图细节**。不同tile间的特征交互不足，会牺牲部分全局信息，会适合那些空间局部性强的任务。

---
### 2 Packing 

Packing技术是Vision Transformer（ViT）中处理**多分辨率/多宽高比图像**的核心创新，由Google在2023年论文《[Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/abs/2307.06304)》中提出。其核心目标是**消除传统padding带来的计算冗余**，实现高效并行训练。以下是对该技术的清晰解析：
**核心原则**：  
> **"不通过padding对齐长度，而是将所有图像的序列直接拼接成超长序列，并用block-diagonal mask隔离不同图像的注意力计算"**

#### **关键步骤**
1. **Patchify与位置编码**  
   - 每张图像 $I_k$ 被分割为序列 $s_k \in \mathbb{R}^{L_k \times d}$（$L_k = \frac{H_k}{s} \times \frac{W_k}{s}$，$s$为patch大小）。
   - 添加位置编码得到 $p_k \in \mathbb{R}^{L_k \times d}$（支持加性/乘性编码，如2D-ROPE）。

2. **序列拼接（Packing）**  
   - 将所有 $p_k$ 沿序列维度拼接：  
     $$
     x = \text{Cat}([p_1, p_2, \dots, p_m], \text{dim}=0) \in \mathbb{R}^{\sum L_k \times d}
     $$
   - 得到**单个超长序列** $x$（总长度 $L_{\text{total}} = \sum_{k=1}^m L_k$）。

3. **Block-Diagonal Attention Mask**  
   - **关键约束**：在Attention计算中引入二值掩码 $M$，确保：  
     - **同一图像内**的token可相互注意力（$M_{ij}=1$）。
     - **跨图像**的token禁止注意力（$M_{ij}=0$）。
   - 数学形式：  
     $$
     \text{Attention}(Q,K,V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}} + M\right) V
     $$
     其中 $M$ 是分块对角矩阵（如下图）。

4. **结果分离**  
   - 将Transformer输出按原始序列长度分割：  
     $$
     [p_1', \dots, p_m'] = \text{Split}(\text{Transformer}(x), \text{dim}=0)
     $$
   - 每个 $p_k'$ 对应图像 $I_k$ 的有效特征。

---

### **Why Block-Diagonal Mask？**
- **若不加约束**：Transformer的全局注意力会使**不同图像的token相互混合**（如图中虚线），导致语义混淆（e.g., 猫的patch关注狗的patch）。
- **Mask的作用**：  
  - 严格限制注意力仅在**同一图像的token内部**计算。
  - **等价性保证**：计算结果与"单独处理每张图像"完全一致，但避免了padding开销。


---

### **可能要注意的点**
- **位置编码兼容性**：  
  - 加性位置编码（直接相加）可直接使用。
  - 乘性位置编码（如2D-ROPE）需确保**跨图像位置不连续**（避免位置索引冲突）。
- **工程优化**：  
  - 动态batching：按分辨率相似度分组，减少 $\sum L_k$ 的波动。
  - 梯度同步：需对分离后的特征 $p_k'$ 单独计算损失。

---

### **应用**
- **任意分辨率训练**：无需预处理resize，保留原始图像比例。
- **多模态扩展**：为ViT与语言模型（如LLaVA）的联合训练提供高效batching方案。
- **工业级落地**：Google已在Gemini等模型中实践该技术，显著降低训练成本。

---
### 对比

tilling 将图像映射到预设分辨率模板，切分固定大小tile，加上缩略图捕获全局信息。计算量较小，但是tile之间的交互可能不充分。
packing直接拼接成一个长序列，需要修改 attention mask 与位置编码逻辑，在超长序列上显存占用大。