LLaVA-NeXT  https://llava-vl.github.io/blog/2024-01-30-llava-next/
LLaVA 的 AnyRes 技术旨在通过动态处理高分辨率图像来提升模型对图像细节的感知能力，减少因低分辨率导致的“幻觉”（hallucination）。
1.  **固定网格配置**：与 UReader 的自适应选择不同，LLaVA-1.5 采用了一组预设的、固定的网格配置来处理高分辨率图像。这些配置包括 `{2×2, 1×{2,3,4}, {2,3,4}×1}`。这意味着图像会被固定地分割成 2x2、1x2、1x3、1x4、2x1、3x1 或 4x1 等不同布局的子图像块。

2.  **高分辨率输入**：原始图像以高分辨率输入，避免了简单缩放导致的文本模糊或失真，从而保留了更多细节。

3.  **网格化分割**：根据预设的规则或配置，将高分辨率图像分割成多个子图像（tiles）。例如，一个 2x2 的网格会将图像分成四个象限。

4.  **位置编码与融合**：LLaVA 将空间位置信息（如每个子图像在网格中的坐标）编码并融入视觉特征中，以便语言模型理解不同子图像之间的空间关系。
---


DeepSeek-VL2	2×2、3×3 等规则网格切图 + 全局缩略图	固定 384×384 tile；支持 1×1 到 3×3 网格

---

NaViT (Google)	原生可变分辨率 ViT，无需 resize	把图像拆成任意大小的 patch 后直接 pack 成一个长序列	

---

LLaVA-Mini arXiv:2501.03895	只用一个 vision token 的极端压缩	高分辨率图切成 4 个子图（将高分辨率图像在水平和垂直方向上各切一刀，平均分割成 2x2 = 4 个子图像），各子图编码后再融合	

---

UReader  arXiv:2310.05126	
Shape-Adaptive Cropping Module
1.  **预定义网格**：系统预先定义了多种形状（行数 `nh` 和列数 `nw` 不同）的网格，网格总数 `nh·nw` 不超过最大单元数 `Nc`。
2.  **网格匹配**：针对输入图像，通过计算分辨率相关和分辨率无关的交并比（Srr 和 Sra），选择一个能最大程度保持图像分辨率并适应其宽高比的最佳网格 。匹配得分是 Sra 和 Srr 的和。
3.  **图像裁剪与缩放**：将输入图像缩放到所选网格对应的整体尺寸 `(nhHv, nwWv)`，然后裁剪成 `nh·nw` 个局部子图像。同时，为了保留全局结构信息，还会将原图单独缩放到模型的原始分辨率 `(Hv, Wv)` 作为一个全局图像。
4.  **特征提取与融合**：所有局部子图像和全局图像并行地通过视觉编码器和视觉抽象器，提取视觉特征。
5.  **位置编码**：为了使语言模型能够理解多个子图像之间的空间关系，引入了二维的裁剪位置编码。为每个网格单元分配一个位置索引 `(i, j)`，并通过行、列嵌入层生成位置嵌入，并将其添加到对应的视觉特征上，使模型具备空间感知能力。

--------------------------  

###  1 Tiling  

**Tiling是解决Any Resolution问题的一种经典方案**，代表性工作包括UReader、LLaVA-Next、InternVL 1.5、BLIP-3等。其核心思想是通过**预设固定尺寸的子图（tile）与分辨率模板**，将任意分辨率图像转化为可并行处理的标准化输入，具体实现如下：  

---

#### **核心流程**  
1. **预设参数定义**  
   - **Tile尺寸**：固定子图分辨率（如 `384×384`）  
   - **尺度模板**：预设不同宽高比组合（比如`1:1`, `1:2`, `2:1`, `2:2`），对应不同分辨率：  
   

2. **图像适配与切分**  
   - **Step 1: 分辨率映射**  
     将输入图像匹配到**最接近的预设分辨率**（如输入 `500×800` → 映射至 `384×768`）。  
     → *图示：原始图像按宽高比缩放至目标分辨率（如1:2模板）*  
   - **Step 2: Tile切分**  
     将映射后的图像切分为多个 `384×384` 子图（如 `384×768` → 切为2个子图）。  
     → *图示：图像被网格化切分为固定尺寸子图（含边界填充处理）*  
   - **Step 3: 增补全局信息**  
     将原图**整体缩放至 `384×384`**，作为"全局摘要图"与子图拼接。  

3. **并行化输入**  
   - 将所有子图 + 全局摘要图在 **Batch维度拼接**，一次性输入ViT：  
     $$ \text{Input} = [\text{Tile}_1, \text{Tile}_2, ..., \text{Tile}_n, \text{Global}_{384×384}] $$  
     → *图示：子图与全局图并行送入ViT，输出特征向量拼接*  

---

#### **本质与特性**  
- **本质**：将任意分辨率图像**强制映射到预设分辨率网格**，通过切分实现"伪任意分辨率"处理，**仍属于固定形状的Tokenization机制**（依赖预设tile尺寸）。  
- **核心目标**：突破ViT对固定输入分辨率的限制，**实现多分辨率图像的并行推理**。  

---

#### **局限**  

在捕获全局信息上，仅仅是用低分辨率图（如 `384×384`）作为全局信息，**无法有效保留原图细节**。不同tile间的特征交互不足，会牺牲部分全局信息，会适合那些空间局部性强的任务。

---
### 2 Packing 

Packing技术是Vision Transformer（ViT）中处理**多分辨率/多宽高比图像**的核心创新，由Google在2023年论文《[Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/abs/2307.06304)》中提出。其核心目标是**消除传统padding带来的计算冗余**，实现高效并行训练。以下是对该技术的清晰解析：
**核心原则**：  
> **"不通过padding对齐长度，而是将所有图像的序列直接拼接成超长序列，并用block-diagonal mask隔离不同图像的注意力计算"**

#### **关键步骤**
1. **Patchify与位置编码**  
   - 每张图像 $I_k$ 被分割为序列 $s_k \in \mathbb{R}^{L_k \times d}$（$L_k = \frac{H_k}{s} \times \frac{W_k}{s}$，$s$为patch大小）。
   - 添加位置编码得到 $p_k \in \mathbb{R}^{L_k \times d}$（支持加性/乘性编码，如2D-ROPE）。

2. **序列拼接（Packing）**  
   - 将所有 $p_k$ 沿序列维度拼接：  
     $$
     x = \text{Cat}([p_1, p_2, \dots, p_m], \text{dim}=0) \in \mathbb{R}^{\sum L_k \times d}
     $$
   - 得到**单个超长序列** $x$（总长度 $L_{\text{total}} = \sum_{k=1}^m L_k$）。

3. **Block-Diagonal Attention Mask**  
   - **关键约束**：在Attention计算中引入二值掩码 $M$，确保：  
     - **同一图像内**的token可相互注意力（$M_{ij}=1$）。
     - **跨图像**的token禁止注意力（$M_{ij}=0$）。
   - 数学形式：  
     $$
     \text{Attention}(Q,K,V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}} + M\right) V
     $$
     其中 $M$ 是分块对角矩阵（如下图）。

4. **结果分离**  
   - 将Transformer输出按原始序列长度分割：  
     $$
     [p_1', \dots, p_m'] = \text{Split}(\text{Transformer}(x), \text{dim}=0)
     $$
   - 每个 $p_k'$ 对应图像 $I_k$ 的有效特征。

---

### **Why Block-Diagonal Mask？**
- **若不加约束**：Transformer的全局注意力会使**不同图像的token相互混合**（如图中虚线），导致语义混淆（e.g., 猫的patch关注狗的patch）。
- **Mask的作用**：  
  - 严格限制注意力仅在**同一图像的token内部**计算。
  - **等价性保证**：计算结果与"单独处理每张图像"完全一致，但避免了padding开销。


---

### **可能要注意的点**
- **位置编码兼容性**：  
  - 加性位置编码（直接相加）可直接使用。
  - 乘性位置编码（如2D-ROPE）需确保**跨图像位置不连续**（避免位置索引冲突）。
- **工程优化**：  
  - 动态batching：按分辨率相似度分组，减少 $\sum L_k$ 的波动。
  - 梯度同步：需对分离后的特征 $p_k'$ 单独计算损失。

---

### **应用**
- **任意分辨率训练**：无需预处理resize，保留原始图像比例。
- **多模态扩展**：为ViT与语言模型（如LLaVA）的联合训练提供高效batching方案。
- **工业级落地**：Google已在Gemini等模型中实践该技术，显著降低训练成本。

---
### 对比

tilling 将图像映射到预设分辨率模板，切分固定大小tile，加上缩略图捕获全局信息。计算量较小，但是tile之间的交互可能不充分。
packing直接拼接成一个长序列，需要修改 attention mask 与位置编码逻辑，在超长序列上显存占用大。