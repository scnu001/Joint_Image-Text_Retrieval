Towards Improved Text-Aligned Codebook Learning:  Multi-Hierarchical Codebook-Text Alignment with Long Text
arXiv:2503.01261
句子、短语、词语 通过图像编码器不同深浅层数，多层次与图像的对齐


---
Wasserstein距离(用于衡量两个概率分布或一组加权对象之间的距离。)


---

### 多粒度文本编码

1.  **文本切分工具**：
    首先，论文使用一个名为 **TextBlob** 的文本处理工具。这个工具能够自动化地完成一些基本的自然语言处理（NLP）任务，其中就包括**句子和短语的提取**。
    * **句子提取**：将一整段长文本分解成一个个独立的句子。
    * **短语提取**：识别并提取文本中的名词短语（noun phrases）等有意义的短语。

2.  **编码模型：预训练的 BERT**：
    在切分之后，文章利用一个强大的**预训练 BERT 语言模型**来为这些不同粒度的文本片段生成向量表示。

3.  **分层次编码**：
    * **句子粒度（$t_s$）**：
        对于从长文本中提取出的每一个句子，将其作为单独的输入，喂给 BERT 模型。BERT 会对整个句子进行编码，通常会使用 `[CLS]` token 的输出来作为整个句子的语义表示向量。这样，一个文本块就被表示为一个句子向量的集合 $t_s = \{s_1, s_2, \dots, s_{|t_s|}\}$。
    * **短语粒度（$t_p$）**：
        对于提取出的每个短语，也使用 BERT 进行编码，得到它们的向量表示。这形成一个短语向量的集合 $t_p = \{p_1, p_2, \dots, p_{|t_p|}\}$。
    * **词语粒度（$t_w$）**：
        为了专注于与视觉更相关的语义，作者没有对所有词语进行编码。而是**有选择地**提取了视觉相关的词，比如**名词、形容词和量词**。然后，他们直接使用**BERT 预训练好的词嵌入（word embedding）**作为这些词语的向量表示。这形成一个词语向量的集合 $t_w = \{w_1, w_2, \dots, w_{|t_w|}\}$。

通过以上步骤，一个单一的长文本被有效地分解并编码成了三个不同粒度的向量集合：
* **$t_s$**：包含了文本的**宏观语义**（句子级别）。
* **$t_p$**：包含了文本的**局部语义**（短语级别）。
* **$t_w$**：包含了文本的**细粒度语义**（词语级别）。

这样做的好处是，模型在进行语义对齐时，可以利用这三种不同层次的文本信息，与图像编码器产生的低、中、高三个层次的视觉特征一一对应，从而实现更精确的对齐。

---
论文中的分层图像编码器是如何实现输出三个不同层次特征网格的？原文是这样描述的：

---

### 对齐分层图像编码器



> "Given an image, we first encode it into three hierarchical grid features, denoted as **$Z_{f1}$**, **$Z_{f2}$**, and **$Z_{f3}$**, where $Z_{fj} \in R^{\frac{H}{f_j} \times \frac{W}{f_j} \times d_z}$ and $f_j \in \{4, 8, 16\}$... Here, the $Z_{f1}$ captures the feature from the lower layers, while $Z_{f3}$ represents the feature from deeper layers."


1.  **分层特征网格**：图像首先被编码成三个层次的特征网格，分别命名为 $Z_{f1}$、$Z_{f2}$ 和 $Z_{f3}$。
2.  **不同的下采样因子**：这些特征网格是通过使用不同的**下采样因子** $f_j$ 从编码器的不同层中提取出来的。下采样因子 $f_j$ 的值分别为 $\{4, 8, 16\}$。
    * **$f_1 = 4$**：这意味着特征网格 $Z_{f1}$ 的分辨率是原始图像的 $\frac{1}{4}$。这是一个相对较高的分辨率，因为它经过的下采样次数最少。因此，$Z_{f1}$ 包含了更多原始的、局部的像素信息，代表了**低层特征**。
    * **$f_2 = 8$**：$Z_{f2}$ 的分辨率是原始图像的 $\frac{1}{8}$。它经过了更多的下采样，因此比 $Z_{f1}$ 更加抽象，代表了**中层特征**。
    * **$f_3 = 16$**：$Z_{f3}$ 的分辨率是原始图像的 $\frac{1}{16}$。这是最低的分辨率，因为它经过了最多次的下采样。因此，$Z_{f3}$ 的特征最抽象，包含了图像的**高层语义信息**。
3.  **编码器结构**：在CNN中，随着层数的加深，下采样操作（如池化或带步长的卷积）会逐渐减少特征图的分辨率，同时增加其通道数来捕捉更抽象的语义。论文正是利用了这种机制，从不同下采样率的中间层提取特征。
好的，这张图片（Figure 2）详细展示了论文提出的 **TA-VQ** 框架的整体结构。我们可以分步来讲解它，结合图中的箭头和模块，可以更直观地理解整个流程。



* **量化器 (Quantizer)**：量化器 $Q(·)$ 将这些特征网格与**码本（Codebook）**进行匹配，生成离散的码表示 $Z_{f1}, Z_{f2}, Z_{f3}$。
* **基于采样的对齐模块 (Sampling-based Alignment)**：这是连接图像和文本的关键。它执行以下对齐操作：
    * **$Z_{f1}$** (低层图像码) 与 **$t_w$** (词语) 对齐。
    * **$Z_{f2}$** (中层图像码) 与 **$t_p$** (短语) 对齐。
    * **$Z_{f3}$** (高层图像码) 与 **$t_s$** (句子) 对齐。
    这个模块使用像 Wasserstein 距离这样的方法来衡量和最小化图像码和文本向量集合之间的距离，从而实现精确的语义对齐。
* **解码器 (Decoder)**：最后，量化后的高层码表示 $Z_{f3}$ 被送入解码器 $D_{θ_d}(·)$，尝试**重构**出原始图像 $\tilde{x}$。重构损失是整个框架的一个重要组成部分。
（句子级别的语义信息帮助最大）
---

### 总结图中的关键连接

* **图像编码**：图像 $x$ → 多层次编码器 → 量化器 → 层次化码表示 ($Z_{f1}, Z_{f2}, Z_{f3}$)。
* **文本编码**：图像 $x$ → ShareGPT4V → 长文本 → 预训练 BERT → 层次化文本表示 ($t_w, t_p, t_s$ )。
* **多层次对齐**：通过“Sampling-based Alignment”模块，图像的三个层次码表示分别与文本的三个粒度表示进行匹配。
* **重构**：图像的最高层码表示 $Z_{f3}$ 被用于解码器，以重建原始图像。

整个框架通过这种端到端的方式，将图像的视觉特征和文本的语义特征在多个层次上紧密地对齐，从而使得学习到的视觉码本能更好地捕捉到与文本相符的语义信息。

---
### 可能的实现（想法）



1.  **图像编码**：
    * **低层特征**：从图像编码器（ ViT 的）提取**高分辨率的特征图**。这些特征图上的每个向量都对应图像中的一个局部区域。视为一个**连续的局部特征集合**。
    * **高层特征**：从图像编码器的深层提取**低分辨率、高语义的特征向量**，例如 ViT 的 `[CLS]` token 的输出。这个单一向量代表了图像的整体语义。
    * **融合标题**：将标题用一个文本编码器（例如 BERT）编码，得到一个标题向量。可以将它与图像的高层特征向量进行融合，形成一个多模态的**高层查询向量**。

2.  **文本块的多层次编码**：
    * 使用一个预训练的语言模型（例如 BERT 或 RoBERTa）来编码每个文本块。
    * **词语/短语层**：从语言模型的输出中，你可以提取出**每个词语的上下文嵌入向量**。你可以将这些向量的集合视为文本块的**低层表示**。
    * **句子层**：如果文本块由多个句子组成，你可以提取出每个句子的嵌入向量（例如使用 `[CLS]` token），将它们作为一个**句子向量集合**，作为文本块的**高层表示**。

### 多层次对齐和相似度计算


1.  **低层对齐（细粒度）**：
    * 计算图像的**局部特征集合**（来自图像编码器浅层）与文本块的**词语向量集合**之间的相似度。
    * **Wasserstein 距离**。它可以衡量这两个点集之间的最小传输成本，从而判断图像中的局部细节和文本中的关键词是否匹配。

2.  **高层对齐（粗粒度）**：
    * 计算**融合了标题的高层查询向量**与文本块的**句子向量集合**之间的相似度。
    * 在这里使用**余弦相似度**（句子向量集合求平均）或者**Wasserstein 距离**来衡量它们之间的相似度。

3.  **最终相似度**：
    * 将低层对齐的分数和高层对齐的分数进行**加权求和**，得到一个综合的最终相似度分数。例如：
        $$\text{Similarity}(Query, TextBlock) = \alpha \cdot \text{Sim}_{\text{low}} + \beta \cdot \text{Sim}_{\text{high}}$$
    * 通过调整 $\alpha$ 和 $\beta$ 这两个超参数，控制模型是更注重细节匹配还是整体语义匹配。



