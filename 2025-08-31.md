# 类 Hi-Transformer
## 目的

* 保留“词→句→块”两级层次，对每个文本块 $b$ 产出一个归一化向量 $B_b \in \mathbb{R}^{d}$，可直接用于检索（与图像/标题做余弦匹配）。


---

## 输入与预处理

### 块与句子的切割

* 将论文正文划分为块集合 $\{b_m\}_{m=1}^{M}$（按段落/小节）。
* 对每个块 $b$，做句子切分：$\{s_1,\dots,s_{n_b}\}$（句号/换行/标题规则）。
* 每句做分词（与底座 LM 一致：BPE/WordPiece），得到 token 序列长度 $L_j$。

### 长度限制
可能并不需要
* 句子token上限，超过则滑窗并在句子级汇合。
* 每块最多句子数。

### 结构与版面特征

* 每句附带位置与角色 embedding：段内索引 idx、是否含“Fig./图/Table”锚词、是否为标题行。
* 这些特征将在块内 Transformer 的输入处相加。

---

## 词 → 句 编码（Sentence Encoder）


* 选择预训练 嵌入模型。对每个句子 $s_j$ 编码得 token 表示：

$$
T_j = [t_{j,1},\dots,t_{j,L_j}],\quad t_{j,i}\in\mathbb{R}^{d_{\text{lm}}}.
$$

### 句级聚合（三选一，建议用注意力池化）

* **Mean**：$u_j=\frac{1}{L_j}\sum_i t_{j,i}$ 快速并且稳定。
* **\[CLS]**：取嵌入模型的的 \[CLS] 作为 $u_j$。
* **Attention Pooling**：

$$
\alpha_{j,i}=\frac{\exp(w^\top t_{j,i})}{\sum_{k}\exp(w^\top t_{j,k})},\quad
u_j=\sum_i \alpha_{j,i} t_{j,i}.
$$

* 增加一个线性层对齐维度：$\tilde u_j = W_s u_j \in \mathbb{R}^{d}$。

> 冻结嵌入模型大部分层，只微调最后 2–4 层与聚合头。

---

## 句→块 编码（Block Encoder）
Block Encoder 通过 句子向量 + 角色位置嵌入 → 轻量块内 Transformer → 注意力池化（结合先验） → 投影归一化 的流程，将若干句子动态融合为一个稳定的块级语义表示 $B_b$。这样不仅保持了块内部句间关系，还对图表相关内容进行了显式强调，使得块向量能够更好地与视觉 tile 或 caption 表示对齐

### 输入构造

* 句序列矩阵：$U=[\tilde u_1,\dots,\tilde u_{n_b}]^\top \in \mathbb{R}^{n_b\times d}$。
* 位置/角色嵌入：为每句构造 $$
p_j = v_{\text{pos}} + v_{\text{fig}} + v_{\text{title}} + v_{\text{type}} \in \mathbb{R}^d$$

段内相对位置、是否含图表锚词、标题标记等）。
* 输入 token：$x_j=\tilde u_j + p_j$。

### 轻量块内 Transformer

* 层数 $L_B=2\sim4$、多头注意力，带前馈与残差：

$$
X^{(0)}=[x_1,\dots,x_{n_b}],\quad
X^{(\ell+1)}=\mathrm{TransformerLayer}(X^{(\ell)}),\ \ell=0\dots L_B-1.
$$

### 块级聚合（Attention Pooling）

* 定义可学习查询向量 $q\in\mathbb{R}^{d}$，对最后层句表示 $H=X^{(L_B)}$ 做注意力池化：

$$
e_j=\frac{q^\top H_j}{\sqrt{d}} + \beta \cdot r_j,\quad
\omega_j=\frac{\exp(e_j)}{\sum_k \exp(e_k)},\quad
h_b=\sum_{j=1}^{n_b}\omega_j H_j.
$$

其中 $r_j$ 为先验分数（例如句含“Fig./Table/图”等锚词则 $r_j=1$ 否则 0，或用 TF-IDF 句重要性），$\beta$ 控制先验强度（如 0.5）。

### 投影与归一化（最终块向量）

$$
B_b=\frac{W_b h_b}{\|W_b h_b\|_2}\in\mathbb{R}^{d}.
$$

---


### Query感知门控

* 若在线检索时有查询向量 $Q$（可来自 caption 或视觉全局向量），在 3.3 的注意力里让 $q$ 由 $Q$ 生成：

$$
q=\mathrm{MLP}_q(Q)\quad(\text{若无 }Q,\ \text{用全局可学习 }q_0) .
$$

* 这会让 $B_b$ 更贴合当前查询，但离线索引时通常用 $q_0$ 产出的“通用块向量”。
就是查询向量更新块内的句子注意力权重，更新文本块向量，更贴合查询，放大与查询相关的信息。

### 多尺度残差
将句子级别信息融合进块向量中，保留少量句级细节。
* 把句级 pooled（如 $\bar u=\frac{1}{n_b}\sum_j \tilde u_j$）通过门控残差并入：

$$
g=\sigma(w_g^\top [h_b;\bar u]) ,\quad
h_b' = h_b + g \cdot \bar u,\quad
B_b=\frac{W_b h_b'}{\|W_b h_b'\|_2}.
$$

* 这样在只返回一个 $B_b$ 的情况下，也保留了少量句级细节。

---

## 损失与训练

> 文本自监督加跨模态对比损失。

### 文本自监督

* **句↔块 对比（InfoNCE）**：鼓励同块句子与其块向量接近、与他块远离：

$$
\mathcal{L}_{\text{s2b}}
=-\sum_{(b,j)}\log\frac{\exp(\tilde u_j^\top B_b/\tau)}{\sum_{b'}\exp(\tilde u_j^\top B_{b'}/\tau)}.
$$

* **顺序/置乱判别**：对块内句子做置乱，让块内 Transformer 预测是否正确顺序（Binary CE）。
* **Span/Token Masking**：在句内做 MLM，巩固词级语义（可只训练聚合头与少量 LM 顶层）。

### 跨模态对比

* **Caption↔块**：

$$
\mathcal{L}_{\text{c2b}}
=-\sum_{(c,b^+)}\log\frac{\exp(C^\top B_{b^+}/\tau)}{\sum_b \exp(C^\top B_b/\tau)}.
$$

* **图像↔块**。
* **多正样本**：一个图/标题可对应多个正块，用 $\log \sum \exp$ 聚合正例。

### 总损失

$$
\mathcal{L}=\lambda_1 \mathcal{L}_{\text{s2b}}+\lambda_2 \mathcal{L}_{\text{c2b}}+\lambda_3 \mathcal{L}_{\text{MLM}}+\lambda_4 \mathcal{L}_{\text{order}} .
$$



---


## 消融

* 聚合方式：Mean vs \[CLS] vs Attention（比较 Recall\@k/NDCG）。
* 先验偏置 $\beta$：0/0.3/0.5/1.0。
* 块内层数 $L_B$：2/3/4。
* 句长上限与滑窗 stride：影响召回与速度的折中。
* 是否使用Query感知门控：看重排提升与延迟。

