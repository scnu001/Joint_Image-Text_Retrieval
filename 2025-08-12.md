Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking
arXiv:2506.09944
https://github.com/princeton-pli/QRHead
免训练，通过QR-score标记注意力头检索能力，实现检索头选择

---

### **QRScore 计算的三个步骤**

整个计算过程分为三个步骤，对应着论文中的两个公式：

#### **1. 计算单个查询词元到单个文档词元的注意力权重**

这是最基础的单元。在语言模型内部，每个注意力头都会计算一个**注意力权重矩阵**。矩阵中的每个值 $A_{t_q \to t_d}^h$ 都代表注意力头 $h$ 从查询中的某个词元 $t_q$ 转移到文档中的某个词元 $t_d$ 的注意力强度。这个值是经过 softmax 归一化后的结果，范围在0到1之间。

#### **2. 计算单个查询对单个文档的QRScore**

这是公式 (2) 的部分。为了得到查询 $q$ 对文档 $d_i$ 的检索分数，我们需要将所有查询词元到这个文档所有词元的注意力权重都加起来，然后再除以查询的长度 $|q|$ 进行归一化。

$$\text{QRscore}_h(q, d_i) = \frac{1}{|q|} \sum_{t_q \in q} \sum_{t_d \in d_i} A_{t_q \to t_d}^h$$

这个公式可以理解为：
- $\sum_{t_q \in q}$：遍历查询中的每一个词元。
- $\sum_{t_d \in d_i}$：遍历文档 $d_i$ 中的每一个词元。
- $\sum_{t_q \in q} \sum_{t_d \in d_i} A_{t_q \to t_d}^h$：计算查询中的所有词元**共同指向**文档 $d_i$ 的所有词元的总注意力。
- $\frac{1}{|q|}$：用查询的词元数 $|q|$ 进行平均，确保分数不会因为查询长度不同而产生偏差。

这个结果就是注意力头 $h$ 在处理查询 $q$ 时，**对文档 $d_i$ 的平均关注度**。

#### **3. 计算单个注意力头在整个任务上的总QRScore**

这是公式 (3) 和 (4) 的部分。为了评估一个注意力头的整体检索能力，我们需要将它在**所有黄金文档**上的表现进行汇总。

首先，对于一个查询 $q$，我们只关心它对**黄金文档集 $D^*[q]$** 的注意力。公式 (3) 将查询 $q$ 对所有黄金文档的注意力总和起来：

$$\text{QRscore}_h(q) = \frac{1}{|q|} \sum_{d_i \in D^*} \sum_{t_q \in q} \sum_{t_d \in d_i} A_{t_q \to t_d}^h$$

这里的 $\sum_{d_i \in D^*}$ 意味着将所有黄金文档 $d_i$ 的注意力分数都加起来。这衡量的是一个注意力头对于一个查询，**能多大程度上关注到所有正确的答案文档**。

最后，为了得到一个注意力头 $h$ 在整个**检测数据集 $T$** 上的综合表现，对所有查询的 QRScore 取平均值，得到了公式 (4)：

$$\text{QRscore}_{h,T} = \frac{1}{|T|} \sum_{(q,D,D^*) \in T} \text{QRscore}_h(q)$$

这里的 $|T|$ 是数据集中的查询数量。这个最终的 QRScore 就是用来排序和识别出 **QRHEAD** 的依据。分数越高的注意力头，就越有可能被认定为是一个有效的检索头。


---

### **QRRETRIEVER 方法**

QRRETRIEVER 的核心思想是利用选定的 QRHEADs 的注意力权重来为文档打分。

1.  **得分计算**: 给定一个查询 `q` 和一系列文档 `D`，对于每个文档 `dᵢ`，它的检索得分 `R(q, dᵢ)` 是所有选定的 QRHEADs 的 **QRscore** 的平均值。
    $$R(q, d_i) = \frac{1}{|H_{select}|} \sum_{h \in H_{select}} \text{QRscore}_h(q, d_i)$$
2.  **文档排序**: 文档根据这些检索得分 `R(q, dᵢ)` 进行排序，得分越高越相关。
3.  **校准**: 为了消除语言模型注意力权重中的内在偏差，作者引入了**校准（Calibration）**步骤。他们计算一个基线得分 `R(q_{null}, dᵢ)`，其中 `q_{null}` 是一个空查询（如 "N/A"）。最终的校准得分是 `R(q, dᵢ) - R(q_{null}, dᵢ)`，用这个分数来进行最终排序。


---

Rethinking Visual Layer Selection in Multimodal LLMs
arXiv:2503.08510
关于视觉编码器中不同层次的重要性，以及如何选择合适的层次进行特征融合

---

### 1. 并非越深层越好：中层和倒数第二层表现最佳
* **中层具有巨大潜力**：研究发现，视觉编码器的**中层（middle layers）**在许多任务上表现出色，尤其是在**视觉中心任务**（如空间关系、物体定位）和**细粒度感知**任务上，其性能甚至能超越更深的层。这表明中层保留了更丰富的、未被完全抽象化的细节信息，对某些任务至关重要。
* **倒数第二层是深层中的最佳选择**：传统上人们认为应该使用最后一层视觉表示，但研究发现**最后一层**由于其为图像-文本匹配而进行的特殊优化，反而**抑制了局部细节**，导致其性能不如**倒数第二层**。倒数第二层在**视觉丰富性和文本对齐**之间取得了最佳平衡，使其成为深层中性能最好的选择。这一结论在不同LLM规模下依然成立。

### 2. 数据和模型规模的影响
* **核心结论具有普适性**：中层和倒数第二层的优势，在**不同规模的训练数据集**和**不同大小的LLM**（1.4B, 2.7B, 7B）下都得到了验证。
* **数据规模的增大会缩小差距**：随着训练数据规模的增加，LLM能更好地从深层中提取和利用细粒度信息，使得倒数第二层与中层之间的性能差距有所缩小，但中层的优势依然存在。
* **浅层存在固有缺陷**：浅层由于其表示与文本空间缺乏内在对齐，即使增加大量的特定任务数据（如OCR数据），也无法显著提升其在相关任务上的性能。

---

### 3. 特征融合是提升性能的有效策略
* **融合优于单一层次**：将来自不同层次（尤其是**中层和深层**）的视觉特征进行**拼接融合**，能显著提升模型在各类任务上的表现，包括通用任务、OCR、视觉中心任务和幻觉任务。
* **层选择至关重要**：在进行特征融合时，层的选择直接影响最终性能。基于**层级表示相似度（LRS）分析**来选择代表性层进行融合，其效果优于简单地使用分位数等方法进行选择。
* **浅层不适合所有融合**：在融合中加入浅层特征（如第3层）可能会**损害**模型在**OCR任务**上的性能，因为浅层不具备文本对齐能力。

---

### 4. 幻觉问题的根源
* MLLMs中的**幻觉问题**主要源于**视觉表示的质量**，而不是LLM本身的规模。在不同规模的LLM下，幻觉问题的表现没有显著改善，这进一步强调了提供高质量、信息丰富的视觉表示的重要性，而这正是中层和倒数第二层的优势所在。

不是“越深层越好”，强调了视觉编码器中**中层和倒数第二层**的重要性。它还提出，通过**合理地融合不同层次的视觉特征**，可以构建出性能更优、更鲁棒的多模态模型。



---

Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices
arXiv:2503.06063

---
如何有效地**选择和融合多层视觉特征**来提升性能。文章提出了两种视觉层选择策略和两种特征融合策略，并通过一个轻量级模型 Mini-LLaVA 进行了实验验证。

---

### 1. 视觉层选择策略

文章提出了两种视觉层选择方法来系统性地研究不同层次特征的贡献：
* **基于相似度的选择 (Similarity-based Selection)**：这种方法将视觉编码器的 $N$ 层划分为三个阶段：**开始阶段**（浅层，1到$B$层）、**中间阶段**（中层，$B+1$到$M$层）和**结束阶段**（深层，$M+1$到$N$层）。作者认为，同一阶段的层特征具有相似的属性。他们选择每个阶段的代表性层进行实验，例如，**第3层、第18层和第23层**。
* **基于比例的选择 (Proportion-based Selection)**：这种方法将 $N$ 层简单地划分为**前一半**（1到$N/2$层）和**后一半**（$N/2+1$到$N$层）。这种方法更侧重于比较浅层特征和深层特征的贡献，与之前的一些研究保持一致。

---

### 2. 视觉特征融合策略

文章提出了两种主要的特征融合策略，并进一步细分为**模块化融合**和**直接融合**。
* **内部融合 (Internal Fusion)**：
    * **方法**：将多层视觉特征**直接融入LLM的内部**，通常是在LLM的某个中间层进行。
    * **实现**：通过一个投影层 $P_i$ 将视觉特征 $v_i$ 对齐到LLM的嵌入空间，然后通过一个融合函数 $I$ 将其与LLM的隐藏状态 $h_i$ 结合，得到更新后的状态 $h_i'$。
    * **子策略**：
        * **内部模块化融合 (Internal Modular Fusion)**：使用额外的模块（如**交叉注意力模块**）来整合特征。
        * **内部直接融合 (Internal Direct Fusion)**：通过简单的操作（如**元素级相加**）来整合特征。
    * **缺点**：每一层融合都需要一个独立的投影器，导致**参数量随层数增加而增加**。

* **外部融合 (External Fusion)**：
    * **方法**：在将视觉特征输入LLM**之前**，在LLM的输入层进行融合。
    * **实现**：通过一个投影器 $P$ 将多层视觉特征 $F$ 转换为更新后的视觉令牌 $V'$，然后将 $V'$ 和文本令牌一起输入LLM。
    * **子策略**：
        * **外部模块化融合 (External Modular Fusion)**：使用额外的模块来生成或更新视觉令牌。
        * **外部直接融合 (External Direct Fusion)**：使用简单的操作（如拼接或堆叠）来融合特征。
    * **优点**：每个视觉层集只需要**一个投影器**，因此在处理多层特征时更**参数高效**。

---

### 3. 基础模型：Mini-LLaVA

为了进行实验，作者构建了一个名为 **Mini-LLaVA** 的轻量级 MLLM：
* **结构**：基于 LLaVA-1.5，但将 LLM 从 Vicuna 1.5 (7B) 替换为更小的 **MobileLLaMA 1.4B**。
* **组件**：使用 **CLIP-ViT-L/14** 作为视觉编码器，它有24层。LLM 也有24层。
* **训练**：遵循 LLaVA-1.5 的两阶段训练策略：先进行**预训练**（558K图像-描述对），再进行**指令微调**（665K对话）。在预训练阶段，只训练新的组件（如投影器），而在指令微调阶段，除了视觉编码器之外的所有参数都会被优化。
* **特点**：Mini-LLaVA 在内部融合中采用**逐层对齐**的方法，即视觉编码器第 $i$ 层的特征被融合到LLM的第 $i$ 层。



---

### 1. 实验设置

作者设计了 **Mini-LLaVA** 作为基础模型，它使用视觉编码器第23层的特征。实验将该模型与融合了来自不同层集的特征的模型进行比较。这些层集包括：
* **单一层 (Single)**：{18}
* **双层 (Double)**：{3, 18}
* **三层 (Triple)**：{3, 18, 23}
* **前一半层 (Former)**：{1, ..., 12}
* **后一半层 (Latter)**：{13, ..., 24}
* **所有层 (All)**：{1, ..., 24}

模型在四个基准类别上进行评估：**通用任务（General）**、**OCR**、**视觉中心任务（CV-Centric）**和**幻觉（Hallu）**。

---

### 2. 内部融合消融实验

#### 2.1 模块化融合 (Modular Fusion)
模块化融合通过在LLM内部插入额外的模块（如交叉注意力）来整合视觉特征。
* **层组合探究**：在最常见的**前交叉注意力（pre-cross attention）**融合策略下，作者发现：
    * **浅层特征有助于细节任务**：`Double` 组合（包含浅层第3层和中层第18层）在需要图像细节的任务上表现优于 `Single`（只包含第18层），这表明融合浅层特征对于细粒度任务是有益的。
    * **过多层的局限性**：融合过多层（如 `All`）会导致训练困难，损失曲线难以收敛。这是因为额外的模块引入了大量参数，使得优化变得困难。
    * **融合位置的影响**：在**后一半层（Latter）**插入模块会显著降低性能，可能是因为它干扰了LLM深层已经精炼过的特征。相比之下，在**前一半层（Former）**插入模块能让后续层有机会修正和完善特征。
    * **性能提升有限**：模块化融合的性能提升微弱，只有 `Double` 和 `Triple` 相比基线有轻微提升，其他组合甚至表现更差。
* **不同模块化策略**：比较了前交叉、后交叉和并行三种注意力机制后发现，它们之间的总体性能差异很小，这表明对于多层视觉特征融合而言，**模块化融合策略的选择并不那么重要**。

#### 2.2 直接融合 (Direct Fusion)
直接融合通过更简单的操作（如元素级相加）来整合视觉特征，从而减少额外参数的影响。
* **视觉层增加时的稳定性能**：与模块化融合不同，直接融合在增加视觉层数时表现出**更稳定的性能**，甚至在某些任务上有所提升。这表明直接融合能有效地适应额外的视觉信息，而无需增加复杂的模块或训练参数。
* **对后一半层的适应性更强**：直接融合在**后一半层（Latter）**的表现优于前一半层（Former），尤其是在GQA和TextVQA任务上。这与模块化融合的结论相反。这可能是因为在前一半层，LLM对视觉令牌的注意力更强，直接融合可能会带来更大的干扰。而在后一半层，LLM的注意力较弱，直接融合能更平滑地整合视觉信息。

---

### 3. 外部融合消融实验

外部融合在将视觉令牌输入LLM之前就完成特征融合。
* **性能更强**：无论是模块化还是直接融合，**外部融合**的性能都**优于内部融合**。特别是在通用和幻觉任务上，外部融合的优势更为明显。内部融合的最高性能仅为48.74，而外部融合可以达到49.78甚至49.88。
* **直接融合已足够**：在外部融合中，**直接融合就足以有效地整合多层特征**，增加额外的模块并不能带来显著的性能提升。例如，外部模块化融合中，`All` 的性能反而比 `Former` 低0.69点，而外部直接融合中，简单的平均方法在 `All` 组合上取得了最优结果。
* **模块化融合对层选择更敏感**：与内部融合类似，外部模块化融合的性能对层组合的选择更为敏感，而直接融合则更稳定。
---
本部分对多层视觉特征融合的策略进行了进一步分析，探究了在不同训练数据规模和模型组件下，各种融合方法的表现和泛化能力，并最终总结了有效的实现方案。

***

### 1. 数据规模的影响

为了探究训练数据量对融合性能的影响，作者在三种不同的监督微调（SFT）数据规模下进行了实验：332k、665k（LLaVA-1.5的默认数据量）和737k。实验结果如 **图5** 所示，得出以下结论：

* **外部融合（E+D, E+M）表现稳定**：外部直接融合（E+D）和外部模块化融合（E+M）在数据量为665k或更大时能保持高性能，这表明它们在数据量有限的情况下也能取得良好效果。
* **内部融合依赖大数据**：随着训练数据量的增加，内部融合（I）的性能提升更为显著。这表明，内部融合方法需要更大量的训练数据才能充分发挥其潜力，并可能在数据充足的情况下成为一个有竞争力的选择。

***

### 2. 模型组件的影响

为了评估融合策略的泛化能力，作者将Mini-LLaVA模型的组件替换为更先进的选项：
* **视觉编码器**：从CLIP ViT-L替换为**SigLIP**。
* **LLM**：从1.4B MobileLLaMA替换为**2.7B MobileLLaMA**。

实验结果如 **图6** 所示，得出以下结论：

* **外部融合泛化能力强**：外部融合（External fusion）策略在更换为更先进的组件（SigLIP视觉编码器或更大的2.7B MobileLLaMA）后，性能都持续提升。这证明了外部融合在整合多层视觉特征方面具有出色的**可扩展性**和**泛化能力**。例如，E+D配置的平均分数从基线的49.18提升到了SigLIP下的51.76，再到2.7B LLM下的52.63。
* **内部模块化融合的局限性**：当使用更复杂的SigLIP视觉编码器时，内部模块化融合（I+M）的性能显著下降，平均分仅为43.27。这表明这种方法可能存在**参数效率低**或**容易过拟合**的问题，尤其是在与更复杂的组件结合时。

***

### 3. 结论

#### **问题1：如何更有效地选择视觉层？**
* **最有效的选择方法**：从**开始阶段**（浅层，捕获细节）和**中间阶段**（中层，捕获更抽象的特征）各选择一个代表性特征，并结合**结束阶段**（深层，捕获高层语义和文本对齐）的视觉令牌。
* **融合深层的重复性无益**：将已经被用作视觉令牌的深层特征（如第23层）重复融合到模型中，并不能带来显著提升，甚至可能导致性能下降。
* **仅使用后半部分层有缺陷**：只使用后一半层（Latter）进行融合，在需要细节感知的任务上表现较差，不如包含前半部分层（Former）或所有层（All）的配置。

#### **问题2：如何选择有效的融合策略？**
* **外部融合通常优于内部融合**：在大多数情况下，**外部融合**策略的表现**始终优于内部融合**。
* **内部融合的潜力**：当有**大量训练数据**时，内部融合的性能提升更为显著，在最佳条件下有可能接近外部融合的效果。
* **直接融合更稳定**：**直接融合**（Direct fusion）比模块化融合（Modular fusion）更具**稳定性**和**泛化能力**，因为它引入的参数和变异性更少。

**最终实现方案**：
最有效的融合策略是**外部直接融合**（External Direct Fusion），它在各种设置下都展现出强大的性能和出色的泛化能力。如果训练数据量充足，**内部直接融合**也可以作为一种可行的替代方案。

Multi-Layer Visual Feature Fusion in Multimodal LLMs:
Methods, Analysis, and Best Practices

---











快速剪枝 按信息增益
https://github.com/Theia-4869/CDPruner
文本导向剪枝
https://github.com/Gumpest/SparseVLMs