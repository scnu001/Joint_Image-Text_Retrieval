HowFarAre WetoGPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites
arXiv:2404.16821

预定义不同长宽比，按模板分割成tile，并且送入一张缩略图

https://github.com/OpenGVLab/InternVL

---

## InternVL 1.5 

1. **架构**

   * Vision Encoder: **InternViT-6B-448px**（从 224 提升到 448，并持续预训练）。
   * Language Model: **InternLM2-20B**，通过 MLP projector 与视觉特征连接。
   * 架构形式：标准 **ViT-MLP-LLM**。

2. **动态高分辨率策略**

   * **Tile 基础尺寸**：448×448。
   * **Tile 数量**：训练时最多 12 tiles（≈2688×2688 分辨率），推理时可扩展到 40 tiles（≈4K 输入）。
   * **Aspect Ratio Matching**：

     * 预定义 **35 种长宽比组合**（由 1–12 tiles 组成，如 1:1, 1:2, 2:3, … 2:6）。
     * 选择与输入图像最接近的比例，避免过度拉伸或缩放。
     * 优先避免过度放大低分辨率图像。
   * **全局缩略图**：和局部 tiles 一起输入，保留全局语境。
   * **Pixel Shuffle**：把每个 tile 的 token 数压缩为 **1/4**，即 448×448 → 256 tokens。


---

Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling
arXiv:2412.05271

本质上在tiling中没有过多的改变

---

##  InternVL 2.5

### 1. **整体架构**

* **Vision Encoder**：InternViT-6B (5.5B, 45 层) 或 InternViT-300M (0.3B, 24 层)。

  * 训练损失：Next Token Prediction (NTP)
* **语言模型**：升级为最新的 **InternLM 2.5 / Qwen 2.5** 系列。
* **连接方式**：2 层随机初始化的 MLP projector。
* **Token 压缩**：Pixel Unshuffle 把空间维度的像素块打散到通道维度，实现无信息丢失的下采样 → 每个 448×448 tile 压缩为 **256 visual tokens**（¼）。降低分辨率，增加通道数，可以节省算力

---

### 2. **动态高分辨率策略扩展**

* **延续 InternVL 1.5 的 tiling 思路**：

  * tile 大小固定为 448×448
  * 支持 **closest aspect ratio matching**（避免过度拉伸，优先缩放到接近比例）
  * tile 数目约束在 `[nmin, nmax]`，并可加上全局 thumbnail
* **2.0/2.5 新增**：

  * **多图像输入**：nmax 在多图像样本间分配（Image-1, Image-2 标签）
  * **视频输入**：nmax=1，每帧缩放为 448×448，配合多帧堆叠 (8f\~32f)
  * **数据格式统一**：所有视觉输入统一用 `<img> … </img>` 标签，附加标签区分图像/帧索引。

---

### 3. **训练流程 **

* **Stage 1: MLP Warmup**

  * 只训练 MLP projector，冻结 ViT + LLM
  * 直接启用动态高分辨率策略
  * 高学习率，快速对齐视觉特征与 LLM embedding 空间
* **Stage 1.5: ViT Incremental Learning (可选)**

  * ViT + MLP 可训练
  * 目的：增强视觉特征，适配长尾领域（OCR / 数学图表）
  * 低学习率，避免灾难性遗忘
  * 训练好的 ViT 可在后续复用 → 方便和更大的 LLM 对齐
* **Stage 2: 全模型 Instruction Tuning**

  * ViT + MLP + LLM 联合训练
  * 严格控制数据质量，防止 LLM 被噪声破坏
  * 统一学习率（不再分模块）

---

### 4. **渐进扩展 (Progressive Scaling)**

* 先用小 LLM（1.8B / 7B / 20B）与 ViT 联合训练
* 再迁移到更大 LLM（32B / 72B）→ 避免重复训练 ViT
* 通过共享权重 + NTP 泛化，使 ViT 特征能被不同 LLM 理解

-








---
InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency
arXiv:2508.18265

ViR 根据每个 patch 的语义重要性动态调整 token 数量，实现自适应 tiling。

---

# **InternVL3.5**


---

### **每图像 patch → Visual Tokens → Pixel Shuffle**

> “每图像 patch 1024 tokens → Pixel Shuffle → 256 tokens 输入 LLM”

* 这里的 **patch** 本质上就是一种 tiling：把大图切成小块（patch），每块再生成 token。
* InternVL3.5-Flash 还增加了 **patch-aware compression**，按 patch 的语义重要性选择压缩率（256 → 64 tokens），这也是 **动态 tiling 或 adaptive tiling** 的一种体现。


### **Visual Resolution Router (ViR)**

> “patch router 根据语义丰富度决定压缩率”

* ViR 会判断每个 patch 是否需要高分辨率 token 或可以压缩，这和 tiling 的目的类似：**对重要区域保持细粒度信息，对无关区域降低分辨率/token数**。



---

## **模型架构**

* **遵循 ViT–MLP–LLM 范式**：

  * 视觉编码器：InternViT-300M / InternViT-6B
  * 语言模型：Qwen3 系列 / GPT-OSS
* **视觉 token 压缩**：

  * **InternVL3.5**：每图像 patch 1024 tokens → Pixel Shuffle → 256 tokens 输入 LLM
  * **InternVL3.5-Flash**：

    * 引入 Patch-aware Pixel Shuffle：根据 patch 语义丰富度动态选择压缩率（256 → 64 tokens）
    * 50% tokens 减少，性能几乎不变
* **模型规模示例（Dense / MoE）**：

  * Dense：1B → 38B
  * MoE：20B-A4B → 241B-A28B
  * “A”表示激活参数数量

---

## **训练流程**

### **Pre-training**

* **目标**：统一大规模文本 + 多模态数据训练
* **损失函数**：Next Token Prediction (NTP) + square averaging reweight
* **数据集**：

  * 多模态：图像 captioning、QA、OCR、文档理解、医学等
  * 文本-only：InternLM 系列 + 开源数据
  * 总计：约 116M 样本，250B tokens，text\:multi ≈ 1:2.5
  * 最大序列长度：32k tokens

### **Post-training**

* **三阶段策略**：

  1. **Supervised Fine-Tuning (SFT)**

     * 用高质量对话数据增强下游任务能力
     * 保持 32k context，覆盖 instruction-following、多模态推理、能力扩展（GUI / SVG / 交互）
  2. **Cascade Reinforcement Learning (Cascade RL)**

     * 结合 offline RL (MPO) 和 online RL (GSPO)
     * 优势：训练稳定、效率高、性能上限更高
  3. **Visual Consistency Learning (ViCO)**

     * 集成 ViR，形成 InternVL3.5-Flash
     * 两阶段：

       1. **Consistency Training**：最小化不同视觉压缩率输出的 KL divergence
       2. **Router Training**：训练 ViR 动态选择 patch 压缩率

       * 压缩率 1/4 → 256 tokens，1/16 → 64 tokens



---

##   **Decoupled Vision-Language Deployment (DvD)**


  * 视觉服务器：ViT + MLP (ViR) → 生成 feature embedding
  * 语言服务器：LLM 仅进行 prefilling + decoding
  * **异步流水线**：视觉计算 + feature 传输 + LLM 解码并行执行


---





