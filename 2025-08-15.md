Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling

arXiv:2106.01040

---

###  总体架构概述

Hi-Transformer 是一个 **层次化交互式 Transformer 框架**，一共分为 **三个阶段**：

```
1. 句子内部建模（Sentence Context Modeling）
2. 跨句建模（Document Context Modeling）
3. 全文上下文增强的句子建模（Global Context-enhanced Sentence Modeling）
```

最终通过 **层次化池化（Hierarchical Pooling）**，得到一个文档级别的向量表示。

---

##  模块 1：Sentence Context Modeling

目标：**建模每个句子内部的词与词之间的关系**

### 步骤如下：

* 把整篇文档切分成 M 个句子：

  $$
  \text{Sentence}_i = [w_{i,1}, w_{i,2}, \dots, w_{i,K}]
  $$

* 在每个句子结尾加入一个 `[CLS]` token（记为 $w^s$），用于获取句子的整体表示。

* 对每个句子的词做 **词向量 + 位置向量嵌入**，形成：

  $$
  [e_{i,1}, e_{i,2}, \dots, e_{i,K}, e^s]
  $$

* 输入进 **Sentence Transformer（局部小Transformer）**，得到上下文表示：

  $$
  [h_{i,1}, h_{i,2}, \dots, h_{i,K}, h^s_i]
  $$

* 其中 $h^s_i$ 就是句子 $i$ 的向量表示（通过 \[CLS] token 得到）。

 **目的：每个句子内部建模好词语的语义交互。**

---

## 模块 2：Document Context Modeling

目标：**建模句子之间的关系，得到上下文感知的句子表示**

### 步骤如下：

* 收集所有句子的 `[CLS]` 表示，拼成句子序列：

  $$
  [h^s_1, h^s_2, \dots, h^s_M]
  $$

* 加入**句子级位置编码** $p_i$，让模型知道句子顺序。

* 输入进 **Document Transformer**（句子级的Transformer）中，得到：

  $$
  [r^s_1, r^s_2, \dots, r^s_M]
  $$

* 每个 $r^s_i$ 是考虑了整篇文档上下文后的**句子表示**。

 **目的：通过跨句交互理解全局上下文。**

---

##  模块 3：Global Document Context-Enhanced Sentence Modeling

目标：**利用全局上下文增强每个句子的内部建模**

### 步骤如下：

* 对于每个句子 $i$，把：

  * 原始词表示（从模块1得到）：

    $$
    [h_{i,1}, h_{i,2}, \dots, h_{i,K}]
    $$
  * 加上当前句子对应的全局增强句子向量 $r^s_i$

* 组成新的序列，输入进一个新的 Sentence Transformer，得到：

  $$
  [d_{i,1}, d_{i,2}, \dots, d_{i,K}, d^s_i]
  $$

* 每个词表示 $d_{i,j}$ 同时结合了：

  * 句子内部的语境信息（局部）
  * 文档级上下文信息（全局）

 **目的：双向信息融合，让词语理解具备全文视野。**

---

##  最终表示：Hierarchical Pooling

将以上的 **document-aware word embeddings** 逐级聚合：

1. 对每句话内部的词表示：

   $$
   [d_{i,1}, \dots, d_{i,K}, d^s_i] \Rightarrow \text{sentence embedding } s_i
   $$

2. 把所有句子的表示再次聚合成文档向量：

   $$
   [s_1, s_2, \dots, s_M] \Rightarrow \text{document embedding } d
   $$



---
直接用于