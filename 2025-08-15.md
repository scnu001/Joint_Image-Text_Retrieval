Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling

arXiv:2106.01040

通过使用两个transformer 先建模短语关系得到句子表示 然后再通过句子关系得到含有句子上下文的句子表示
最后将两者都输入到一个transformer中得到一个新的句子表示


---

###  总体架构概述

Hi-Transformer 是一个 **层次化交互式 Transformer 框架**，一共分为 **三个阶段**：

```
1. 句子内部建模（Sentence Context Modeling）
2. 跨句建模（Document Context Modeling）
3. 全文上下文增强的句子建模（Global Context-enhanced Sentence Modeling）
```

最终通过 **层次化池化（Hierarchical Pooling）**，得到一个文档级别的向量表示。
![](image\image4HiTransformer.png)
---

##  模块 1：Sentence Context Modeling

目标：**建模每个句子内部的词与词之间的关系**

### 步骤如下：

* 把整篇文档切分成 M 个句子：

  $$
  \text{Sentence}_i = [w_{i,1}, w_{i,2}, \dots, w_{i,K}]
  $$

* 在每个句子结尾加入一个 `[CLS]` token（记为 $w^s$），用于获取句子的整体表示。

* 对每个句子的词做 **词向量 + 位置向量嵌入**，形成：

  $$
  [e_{i,1}, e_{i,2}, \dots, e_{i,K}, e^s]
  $$

* 输入进 **Sentence Transformer（局部小Transformer）**，得到上下文表示：

  $$
  [h_{i,1}, h_{i,2}, \dots, h_{i,K}, h^s_i]
  $$

* 其中 $h^s_i$ 就是句子 $i$ 的向量表示（通过 \[CLS] token 得到）。

 **目的：每个句子内部建模好词语的语义交互。**

---

## 模块 2：Document Context Modeling

目标：**建模句子之间的关系，得到上下文感知的句子表示**

### 步骤如下：

* 收集所有句子的 `[CLS]` 表示，拼成句子序列：

  $$
  [h^s_1, h^s_2, \dots, h^s_M]
  $$

* 加入**句子级位置编码** $p_i$，让模型知道句子顺序。

* 输入进 **Document Transformer**（句子级的Transformer）中，得到：

  $$
  [r^s_1, r^s_2, \dots, r^s_M]
  $$

* 每个 $r^s_i$ 是考虑了整篇文档上下文后的**句子表示**。

 **目的：通过跨句交互理解全局上下文。**

---

##  模块 3：Global Document Context-Enhanced Sentence Modeling

目标：**利用全局上下文增强每个句子的内部建模**

### 步骤如下：

* 对于每个句子 $i$，把：

  * 原始词表示（从模块1得到）：

    $$
    [h_{i,1}, h_{i,2}, \dots, h_{i,K}]
    $$
  * 加上当前句子对应的全局增强句子向量 $r^s_i$

* 组成新的序列，输入进一个新的 Sentence Transformer，得到：

  $$
  [d_{i,1}, d_{i,2}, \dots, d_{i,K}, d^s_i]
  $$

* 每个词表示 $d_{i,j}$ 同时结合了：

  * 句子内部的语境信息（局部）
  * 文档级上下文信息（全局）

 **目的：双向信息融合，让词语理解具备全文视野。**

---

##  最终表示：Hierarchical Pooling
实际上要经过总计三次transformer，第一次：短语进入模块1，获得句子表示；第二次：句子进入模块2获得建模句子之间关系后的句子表示；第三次：把句子输入进模块3（和模块一类似 但是输入输出相反）更新短语表示；
中间反复进行多次迭代，
最后经过短语pooling获得句子表示，句子pooling获得文档表示；


---
